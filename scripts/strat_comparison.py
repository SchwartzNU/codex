#!/usr/bin/env python3
"""
Compare stratification profiles between 2P cells and EW cells by
interpolating both datasets onto a shared x-axis and computing the
cosine similarity for every 2P×EW pair.

Inputs (defaults point to morph_data/):
  - Two-photon (2P) stratification curves: "strat_2P.mat"
      * MATLAB variable 's' is expected, with per-cell field 'strat_norm'
        (array-like) and an optional cell-type field (e.g. 'cell_type', 'ct', 'type').
  - 2P x-axis: "strat_x.csv" (one value per line; matches 2P strat length)
  - EW stratification JSON: "ac_stratification_profiles_EW.json"
      * Generated by export_ac_data.py, contains {"records": [{"segmentID", "z", "distribution", ...}, ...]}

Process:
  - Use 2P x-axis (from CSV) as reference axis (x_ref).
  - Interpolate each EW profile's provided z-axis directly onto x_ref
    (outside-range -> 0), then renormalize to probability distributions.
  - Ensure 2P profiles match x_ref length; normalize each to sum=1, then convert
    to unit L2 norm for cosine similarity.
  - Compute a cosine similarity matrix (n_2P × n_EW).
  - Order rows by 2P cell type (alphabetical), preserving within-type order.
  - Save matrix to CSV/NPY and a heatmap PNG, plus row/column metadata.

Note:
  - Field names in the 2P .mat may vary; this script tries several options.
  - If SciPy is unavailable, it prints a helpful error message.
"""

from __future__ import annotations

import argparse
import json
import os
from dataclasses import dataclass
from typing import List, Tuple, Dict, Any, Optional
import math

import numpy as np

# Maximum number of x-axis group labels to draw on the heatmap
_MAX_X_LABELS = 20

# Heatmap color scaling configuration (set via CLI)
_COLOR_SCALE = "threshold"  # one of: 'linear', 'log', 'threshold'
_THRESH = 0.9               # threshold for 'threshold' mode
_LOG_EPS = 1e-3             # epsilon for log scale


def _normalize_type_name(name: Optional[str]) -> str:
    s = (str(name) if name is not None else "").strip()
    if not s or s.lower() == "nan":
        return "unknown"
    low = s.casefold()
    # Normalize equivalences
    if low in {"on sac", "on_sac", "onsac", "on sac (starburst)", "on starburst"}:
        return "Starburst"
    return s


def _shorten(label: str, max_len: int = 28) -> str:
    label = str(label)
    return label if len(label) <= max_len else (label[: max_len - 1] + "…")


# ------------------------- IO helpers -------------------------

def _read_x_ref(csv_path: str) -> np.ndarray:
    xs: List[float] = []
    with open(csv_path, "r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                xs.append(float(s))
            except Exception:
                continue
    if not xs:
        raise ValueError(f"No numeric values parsed from {csv_path}")
    return np.asarray(xs, dtype=float)


def _load_2p_mat(mat_path: str) -> List[Dict[str, Any]]:
    """Load MATLAB stratification data; return list of dicts per cell with keys:
       - 'strat': np.ndarray (stratification curve)
       - 'cell_type': str (if available; else 'Unknown')
    """
    try:
        from scipy.io import loadmat  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "SciPy is required to read .mat files. Please install scipy (pip install scipy)."
        ) from e

    m = loadmat(mat_path, squeeze_me=True, struct_as_record=False)
    if "s" not in m:
        raise ValueError(f"Expected variable 's' in {mat_path}; found keys: {list(m.keys())}")
    s = m["s"]

    def mat_struct_to_dict(obj) -> Dict[str, Any]:
        d: Dict[str, Any] = {}
        try:
            fields = getattr(obj, "_fieldnames", None)
            if fields:
                for k in fields:
                    d[k] = getattr(obj, k)
        except Exception:
            pass
        if not d and isinstance(obj, dict):
            d = obj
        return d

    cells: List[Dict[str, Any]] = []
    # s can be a single object or an array-like of objects
    s_iter = []
    if isinstance(s, np.ndarray):
        s_iter = s.ravel().tolist()
    else:
        s_iter = [s]

    for elem in s_iter:
        d = mat_struct_to_dict(elem)
        # Extract stratification vector
        strat = None
        for key in ("strat_norm", "strat", "stratification", "y"):
            v = d.get(key)
            if v is not None:
                strat = np.asarray(v, dtype=float).ravel()
                break
        if strat is None:
            # Try direct attribute if dict conversion failed
            try:
                strat = np.asarray(getattr(elem, "strat_norm"), dtype=float).ravel()
            except Exception:
                pass
        if strat is None:
            # Skip if no usable stratification
            continue

        # Extract cell type label
        cell_type = None
        for key in ("cell_type", "ct", "type", "label", "name"):
            v = d.get(key)
            if v is None:
                try:
                    v = getattr(elem, key)
                except Exception:
                    v = None
            if v is not None:
                try:
                    cell_type = str(np.array(v).tolist())
                except Exception:
                    cell_type = str(v)
                break
        cells.append({"strat": strat, "cell_type": cell_type or "Unknown"})

    if not cells:
        raise ValueError("No stratification records found in 2P .mat file")
    return cells


def _load_ew_json(json_path: str) -> List[Dict[str, Any]]:
    with open(json_path, "r", encoding="utf-8") as f:
        d = json.load(f)
    recs = d.get("records", [])
    out: List[Dict[str, Any]] = []
    for r in recs:
        try:
            z = np.asarray(r.get("z", []), dtype=float).ravel()
            y = np.asarray(r.get("distribution", []), dtype=float).ravel()
            sid = int(r.get("segmentID", -1)) if r.get("segmentID") is not None else -1
            if z.size >= 2 and y.size == z.size:
                out.append({"segmentID": sid, "z": z, "y": y})
        except Exception:
            continue
    # Filter unique by segmentID if present
    seen = set()
    uniq: List[Dict[str, Any]] = []
    for rec in out:
        sid = rec["segmentID"]
        if sid in seen:
            continue
        seen.add(sid)
        uniq.append(rec)
    return uniq


# ------------------------- Processing -------------------------

def _normalize_prob(y: np.ndarray) -> np.ndarray:
    y = np.asarray(y, dtype=float)
    y[~np.isfinite(y)] = 0.0
    y[y < 0] = 0.0
    s = float(np.sum(y))
    if s > 0:
        return y / s
    return y


def _to_unit_l2(y: np.ndarray) -> np.ndarray:
    y = np.asarray(y, dtype=float)
    n = float(np.linalg.norm(y))
    if n > 0:
        return y / n
    return y


def _interpolate_to_ref(x_ref: np.ndarray, x_src: np.ndarray, y_src: np.ndarray) -> np.ndarray:
    # Monotonic increasing required by np.interp; sort if needed
    order = np.argsort(x_src)
    xs = x_src[order]
    ys = y_src[order]
    return np.interp(x_ref, xs, ys, left=0.0, right=0.0)


@dataclass
class MatrixOutputs:
    sim: np.ndarray
    row_types: List[str]
    row_order_idx: List[int]
    col_segids: List[int]
    col_types: List[str]


def build_similarity_matrix(
    mat_2p_path: str,
    x_2p_csv: str,
    ew_json_path: str,
    ew_types_map: Optional[Dict[int, str]] = None,
) -> MatrixOutputs:
    # Load inputs
    x_ref = _read_x_ref(x_2p_csv)
    two_p_cells = _load_2p_mat(mat_2p_path)
    ew_cells = _load_ew_json(ew_json_path)

    # 2P profiles -> align to x_ref if lengths mismatch, then normalize
    x2_min, x2_max = float(np.min(x_ref)), float(np.max(x_ref))
    X2 = []  # normalized + unit-l2 vectors
    types: List[str] = []
    for c in two_p_cells:
        y = np.asarray(c["strat"], dtype=float).ravel()
        # If not matching length, assign an even grid over x_ref range for y and interpolate
        if y.size != x_ref.size:
            # Build a linspace over the same range to approximate original x positions
            x_src = np.linspace(x2_min, x2_max, num=y.size)
            y = _interpolate_to_ref(x_ref, x_src, y)
        y = _normalize_prob(y)
        y = _to_unit_l2(y)
        X2.append(y)
        types.append(_normalize_type_name(c.get("cell_type", "Unknown")))
    X2 = np.vstack(X2)  # shape (n2p, L)

    # EW: interpolate each cell's z directly onto x_ref and normalize
    if not ew_cells:
        raise ValueError("No valid EW stratification records in JSON")

    XE = []
    col_segids: List[int] = []
    for rec in ew_cells:
        z = np.asarray(rec["z"], dtype=float).ravel()
        y = np.asarray(rec["y"], dtype=float).ravel()
        yi = _interpolate_to_ref(x_ref, z, y)
        yi = _normalize_prob(yi)
        yi = _to_unit_l2(yi)
        XE.append(yi)
        col_segids.append(int(rec.get("segmentID", -1)))
    XE = np.vstack(XE)  # shape (nEW, L)

    # Cosine similarity: X2 (n2p,L) vs XE (nEW,L) -> (n2p, nEW)
    sim = X2 @ XE.T

    # Build a consistent cell-type ordering from 2P types via agglomerative clustering
    # 1) Compute per-type centroid vectors in the stratification feature space (unit L2)
    type_to_indices: Dict[str, List[int]] = {}
    for i, t in enumerate(types):
        type_to_indices.setdefault(t or "Unknown", []).append(i)
    type_labels = sorted(type_to_indices.keys())
    centroids: List[np.ndarray] = []
    for t in type_labels:
        idxs = type_to_indices[t]
        C = X2[idxs, :]  # already unit-l2 rows
        m = np.mean(C, axis=0)
        m = _to_unit_l2(_normalize_prob(m))
        centroids.append(m)

    # 2) Agglomerative clustering on cosine distance of centroids to get order
    def _cluster_order(centroids: List[np.ndarray], labels: List[str]) -> List[str]:
        try:
            from scipy.spatial.distance import pdist, squareform  # type: ignore
            from scipy.cluster.hierarchy import linkage, leaves_list  # type: ignore
            if len(centroids) <= 1:
                return labels
            M = np.vstack(centroids)
            # Cosine distance = 1 - cosine similarity (rows already unit-l2)
            D = pdist(M, metric="cosine")
            Z = linkage(D, method="average")
            order_idx = leaves_list(Z).tolist()
            return [labels[i] for i in order_idx]
        except Exception:
            # Fallback to alphabetical if SciPy missing
            return sorted(labels)

    type_labels_ordered = _cluster_order(centroids, type_labels)
    type_order = {t: k for k, t in enumerate(type_labels_ordered)}

    # Order rows by clustered type order, stable within type
    row_idx = list(range(sim.shape[0]))
    row_idx.sort(key=lambda i: (type_order.get(types[i] or "Unknown", 10**9), i))
    sim_sorted = sim[row_idx, :]
    types_sorted = [types[i] for i in row_idx]

    # Determine EW column types if mapping provided
    col_types = [""] * len(col_segids)
    if ew_types_map:
        for j, sid in enumerate(col_segids):
            ct = ew_types_map.get(int(sid))
            col_types[j] = _normalize_type_name(ct or "Unknown")
    else:
        col_types = ["unknown"] * len(col_segids)

    # Order columns by the same type ordering (unknown and types not in 2P go last, alpha within group)
    def col_sort_key(j: int) -> Tuple[int, str, int]:
        ct = col_types[j] or "Unknown"
        idx = type_order.get(ct, 10**9)
        return (idx, ct, int(col_segids[j]))

    col_idx = list(range(len(col_segids)))
    col_idx.sort(key=col_sort_key)

    sim_sorted = sim_sorted[:, col_idx]
    col_segids_sorted = [col_segids[j] for j in col_idx]
    col_types_sorted = [col_types[j] for j in col_idx]

    return MatrixOutputs(
        sim=sim_sorted,
        row_types=types_sorted,
        row_order_idx=row_idx,
        col_segids=col_segids_sorted,
        col_types=col_types_sorted,
    )


# ------------------------- Plot/save -------------------------

def _ensure_outdir(p: str) -> None:
    if not os.path.isdir(p):
        os.makedirs(p, exist_ok=True)


def save_outputs(outdir: str, M: MatrixOutputs) -> Tuple[str, str, str, str, str, str, str]:
    _ensure_outdir(outdir)
    # Save matrix as .npy and .csv
    npy_path = os.path.join(outdir, "cosine_similarity_2P_vs_EW.npy")
    np.save(npy_path, M.sim)

    csv_path = os.path.join(outdir, "cosine_similarity_2P_vs_EW.csv")
    # CSV: header with EW segids; first two columns are 2P_row_index and 2P_cell_type
    import csv
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        header = ["2P_row_index", "2P_cell_type"] + [str(sid) for sid in M.col_segids]
        w.writerow(header)
        # Optional second header row with EW cell types for reference
        w.writerow(["", ""] + [ct for ct in M.col_types])
        for i, (row, ct) in enumerate(zip(M.sim, M.row_types)):
            w.writerow([i, ct] + [f"{v:.6f}" for v in row])

    # Row and column metadata
    row_meta_path = os.path.join(outdir, "rows_2P_types.csv")
    with open(row_meta_path, "w", newline="", encoding="utf-8") as f:
        import csv
        w = csv.writer(f)
        w.writerow(["row_index", "original_index", "cell_type"]) 
        for i, orig in enumerate(M.row_order_idx):
            w.writerow([i, orig, M.row_types[i]])

    col_meta_path = os.path.join(outdir, "cols_EW_segids.csv")
    with open(col_meta_path, "w", newline="", encoding="utf-8") as f:
        import csv
        w = csv.writer(f)
        w.writerow(["col_index", "segmentID", "cell_type"]) 
        for j, (sid, ct) in enumerate(zip(M.col_segids, M.col_types)):
            w.writerow([j, sid, ct])

    # Heatmap
    fig_path = os.path.join(outdir, "cosine_similarity_heatmap.png")
    try:
        import matplotlib.pyplot as plt  # type: ignore
        sim = M.sim
        nrows, ncols = sim.shape
        plt.figure(figsize=(min(18, max(10, ncols/80)), min(12, max(6, nrows/20))))
        # Choose color scaling
        if _COLOR_SCALE == 'log':
            from matplotlib.colors import LogNorm  # type: ignore
            eps = float(_LOG_EPS)
            data = np.clip(sim, eps, 1.0)
            im = plt.imshow(data, aspect='auto', interpolation='nearest', cmap='viridis', norm=LogNorm(vmin=eps, vmax=1.0))
            cbar = plt.colorbar(im, label='Cosine similarity (log scale)')
        elif _COLOR_SCALE == 'threshold':
            thr = float(_THRESH)
            data = np.ma.masked_less(sim, thr)
            cmap = plt.get_cmap('viridis').copy()
            cmap.set_bad(color='#f2f2f2')  # light gray for masked
            im = plt.imshow(data, aspect='auto', interpolation='nearest', cmap=cmap, vmin=thr, vmax=1.0)
            cbar = plt.colorbar(im, label=f'Cosine similarity (>= {thr:g})')
        else:
            im = plt.imshow(sim, aspect='auto', interpolation='nearest', cmap='viridis', vmin=0.0, vmax=1.0)
            cbar = plt.colorbar(im, label='Cosine similarity')
        plt.title('2P vs EW stratification similarity')
        plt.xlabel('EW cells grouped by cell type')
        plt.ylabel('2P cells grouped by cell type')

        # Build group boundaries and midpoints for labeling
        def _groups(labels: List[str]):
            if not labels:
                return []
            groups = []
            start = 0
            cur = labels[0]
            for i in range(1, len(labels) + 1):
                if i == len(labels) or labels[i] != cur:
                    end = i - 1
                    mid = (start + end) / 2.0
                    groups.append((cur, start, end, mid))
                    if i < len(labels):
                        start = i
                        cur = labels[i]
            return groups
        row_groups = _groups(M.row_types)
        col_groups = _groups(M.col_types)

        # Grid lines at group boundaries
        for _, _, r_end, _ in row_groups[:-1]:
            plt.hlines(r_end + 0.5, -0.5, ncols - 0.5, colors='k', linewidth=0.5, alpha=0.3)
        for _, _, c_end, _ in col_groups[:-1]:
            plt.vlines(c_end + 0.5, -0.5, nrows - 0.5, colors='k', linewidth=0.5, alpha=0.3)

        # Tick labels at group centers with cell type names
        if row_groups:
            plt.yticks([g[3] for g in row_groups], [g[0] for g in row_groups], fontsize=7)
        if col_groups:
            # Show ALL group labels, rotated 45 degrees and staggered to reduce overlap
            xticks = [g[3] for g in col_groups]
            xlabels = [g[0] for g in col_groups]
            plt.xticks(xticks, xlabels, rotation=45, fontsize=7)
            ax = plt.gca()
            for i, lbl in enumerate(ax.get_xticklabels()):
                lbl.set_ha('right')
                lbl.set_rotation_mode('anchor')
                # Stagger alternate labels vertically
                y = lbl.get_position()[1]
                lbl.set_y(y - (0.03 if (i % 2) else 0.0))
            # Add extra bottom margin for readability
            plt.gcf().subplots_adjust(bottom=min(0.35, 0.20 + 0.002 * len(col_groups)))

        plt.tight_layout()
        plt.savefig(fig_path, dpi=200)
        plt.close()

        # Also compute and plot means by cell type (row and column groups)
        def _groups(labels: List[str]):
            if not labels:
                return []
            groups = []
            start = 0
            cur = labels[0]
            for i in range(1, len(labels) + 1):
                if i == len(labels) or labels[i] != cur:
                    end = i - 1
                    mid = (start + end) / 2.0
                    groups.append((cur, start, end, mid))
                    if i < len(labels):
                        start = i
                        cur = labels[i]
            return groups

        row_groups = _groups(M.row_types)
        col_groups = _groups(M.col_types)

        # Enforce cluster-consistent ordering for column type groups to match rows
        row_type_order = {g[0]: i for i, g in enumerate(row_groups)}
        col_groups = sorted(col_groups, key=lambda g: (row_type_order.get(g[0], 10**9), g[0]))

        # Build mean matrix in this order
        R, C = len(row_groups), len(col_groups)
        mean_mat = np.zeros((R, C), dtype=float)
        for ir, (_, rs, re, _) in enumerate(row_groups):
            for jc, (_, cs, ce, _) in enumerate(col_groups):
                block = sim[rs:re + 1, cs:ce + 1]
                mean_mat[ir, jc] = float(np.mean(block)) if block.size else np.nan

        # Save means CSV
        means_csv_path = os.path.join(outdir, "cosine_similarity_by_type.csv")
        import csv as _csv
        with open(means_csv_path, "w", newline="", encoding="utf-8") as f:
            w = _csv.writer(f)
            header = ["2P_type \\ EW_type"] + [g[0] for g in col_groups]
            w.writerow(header)
            for ir, g in enumerate(row_groups):
                label = g[0]
                row_vals = [f"{v:.6f}" for v in mean_mat[ir, :]]
                w.writerow([label] + row_vals)

        # Plot means heatmap (always linear scale, no threshold)
        means_fig_path = os.path.join(outdir, "cosine_similarity_heatmap_means_by_type.png")
        plt.figure(figsize=(min(14, max(8, C/6)), min(12, max(6, R/2))))
        im = plt.imshow(mean_mat, aspect='auto', interpolation='nearest', cmap='viridis', vmin=0.0, vmax=1.0)
        cbar = plt.colorbar(im, label='Mean cosine similarity')

        plt.title('2P vs EW similarity (means by cell type)')
        plt.xlabel('EW cell types')
        plt.ylabel('2P cell types')
        plt.yticks(range(R), [g[0] for g in row_groups], fontsize=8)
        plt.xticks(range(C), [g[0] for g in col_groups], rotation=45, fontsize=8, ha='right', rotation_mode='anchor')
        plt.gcf().subplots_adjust(bottom=min(0.35, 0.20 + 0.002 * C))
        plt.tight_layout()
        plt.savefig(means_fig_path, dpi=200)
        plt.close()
    except Exception as e:
        # If matplotlib is missing, skip plotting but keep other outputs
        with open(os.path.join(outdir, "PLOT_ERROR.txt"), "w", encoding="utf-8") as f:
            f.write(f"Plotting failed: {e}\n")
        means_csv_path = os.path.join(outdir, "cosine_similarity_by_type.csv")
        means_fig_path = os.path.join(outdir, "cosine_similarity_heatmap_means_by_type.png")

    return npy_path, csv_path, row_meta_path, col_meta_path, fig_path, means_csv_path, means_fig_path


def _read_ew_types_csv(path: str) -> Dict[int, str]:
    import csv
    mapping: Dict[int, str] = {}
    with open(path, "r", encoding="utf-8") as f:
        rdr = csv.DictReader(f)
        # Expect at least 'segmentID' and 'cell_type' columns
        for row in rdr:
            try:
                sid = int(str(row.get("segmentID", "")).strip())
                ct = str(row.get("cell_type", "")).strip()
                if sid and ct and ct.lower() != "nan":
                    mapping[sid] = ct
            except Exception:
                continue
    return mapping


def main() -> None:
    p = argparse.ArgumentParser(description="Compare 2P and EW stratification profiles via cosine similarity.")
    p.add_argument("--two-photon-mat", default="morph_data/strat_2P.mat", help="Path to 2P MATLAB file with variable 's'")
    p.add_argument("--two-photon-x", default="morph_data/strat_x.csv", help="Path to CSV with x-axis for 2P stratification")
    p.add_argument("--ew-json", default="morph_data/ac_stratification_profiles_EW.json", help="Path to EW stratification JSON")
    p.add_argument("--outdir", default="morph_data/strat_comparison", help="Output directory for matrices and plots")
    p.add_argument("--ew-types-csv", default="", help="Optional CSV mapping with columns segmentID,cell_type to order EW columns by cell type consistent with 2P")
    p.add_argument("--max-x-labels", type=int, default=20, help="Max number of x-axis cell-type labels to draw")
    p.add_argument("--color-scale", choices=["linear", "log", "threshold"], default="threshold", help="Heatmap color scaling")
    p.add_argument("--threshold", type=float, default=0.9, help="Threshold for 'threshold' color scale")
    p.add_argument("--log-eps", type=float, default=1e-3, help="Epsilon for 'log' color scale to avoid log(0)")
    args = p.parse_args()

    # Auto-discover EW types mapping if not provided
    ew_types_map = None
    ew_types_candidate = args.ew_types_csv.strip() or os.path.join(os.path.dirname(args.ew_json), "EW_arbor_stats.csv")
    if ew_types_candidate and os.path.isfile(ew_types_candidate):
        try:
            ew_types_map = _read_ew_types_csv(ew_types_candidate)
            if not ew_types_map:
                print(f"Warning: No usable entries found in {ew_types_candidate}; proceeding without EW types.")
        except Exception as e:
            print(f"Warning: Failed to read EW types CSV ({ew_types_candidate}): {e}")

    M = build_similarity_matrix(args.two_photon_mat, args.two_photon_x, args.ew_json, ew_types_map=ew_types_map)
    # Configure label density for plotting
    global _MAX_X_LABELS
    try:
        _MAX_X_LABELS = max(5, int(args.max_x_labels))
    except Exception:
        _MAX_X_LABELS = 20
    # Configure color scaling
    global _COLOR_SCALE, _THRESH, _LOG_EPS
    _COLOR_SCALE = str(args.color_scale)
    _THRESH = float(args.threshold)
    _LOG_EPS = float(args.log_eps)
    npy_path, csv_path, row_meta_path, col_meta_path, fig_path, means_csv_path, means_fig_path = save_outputs(args.outdir, M)
    print("Done.")
    print(f"  Similarity matrix (npy): {npy_path}")
    print(f"  Similarity matrix (csv): {csv_path}")
    print(f"  2P row metadata: {row_meta_path}")
    print(f"  EW col metadata: {col_meta_path}")
    print(f"  Heatmap: {fig_path}")
    print(f"  Mean-by-type matrix (csv): {means_csv_path}")
    print(f"  Mean-by-type heatmap: {means_fig_path}")


if __name__ == "__main__":
    main()
